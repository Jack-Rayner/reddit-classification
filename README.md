## SubReddit Classification
___
### Background:
As the internet solidifies itself as the primary forum for discussions of virtually every topic, it has become increasingly difficult to differentiate fact and opinion. As the largest online forum and aptly named "front page of the internet", there seems no better candidate for studying this than [Reddit](https://www.reddit.com).
<br>
### Problem Statement:
__This project aims to determine what features make a Reddit post likely to be objective, and what features are characteristic of an opinion post.__
<br>


### Executive Summary:
The impact of these features may be gauged though using them as features for a model to classify Subreddits, and measuring the accuracy of this model. To ensure that any flaws in classification are due to the features rather than shortcomings of the model, both random forest and logistic regression models will be tested. Through careful consideration, the Subreddits selected for analysis and classification were [r/AskScience](https://www.reddit.com/r/askscience/), a forum where science related questions are discussed and answered, and [r/UnpopularOpinion](https://www.reddit.com/r/unpopularopinion/), ostensibly a hotbed for hot takes.  As both of these Subreddits are fairly large (AskScience having upwards of 22 million members), obtaining enough data to analyze and train a model should be fairly straightforward.

#### Pulling the Data:
The data was pulled in [this](https://github.com/Jack-Rayner/reddit-classification/blob/main/Notebooks/Data_Scrape.ipynb) notebook using Pushshift's [reddit API](https://github.com/pushshift/api). As the API required no more than 100 posts be pulled per request, the simplest way to obtain a large amount of data seemed to be to create a while loop. This loop was designed to run until the desired number of posts was reached. In order to do this, an empty DataFrame was created, then immediately overwritten inside the loop by the first 100 posts. All subsequent pulls could then simply be added to this new DataFrame. To ensure that no rows were duplicated between requests, the date and time to begin pulling were set to the the date and time of the post most recently added to the DataFrame. With the data pulled and in a DataFrame, it was important to ensure that no rows were duplicated and none contained missing vales in important columns. While a small number of posts from each SubReddit were duplicated (25 from AskScience and 37 from UnpopularOpinion), this made up less than two percent of the data, so this could be safely dropped with no impact to the model. The DataFrames for each SubReddit were then concatenated and exported to a CSV for Cleaning and EDA.

#### Cleaning and Exploratory Data Analysis
While the data scraped from Reddit contained all the text and information necessary to run a classification model, there was some cleaning necessary to maximize performance and facilitate deeper analysis. This was performed in the [EDA notebook](https://github.com/Jack-Rayner/reddit-classification/blob/main/Notebooks/EDA.ipynb). The first step after ensuring that all the data imported correctly was adding a binary target column. While not strictly necessary, it may aid in filtering the data down the line, and makes explicit what the target to be predicted is. The majority of missing values were in the selftext column, and were either marked as [removed], or left completely blank. As they were text columns, the best option was replacing them with empty strings, as no information is lost, and missing values have no impact on analysis or modeling. With the missing values taken care of, both the title and selftext columns could be merged into a new column containing all the text from each post (appropriately named *all_text*).  In order to have a word count for each post, a lambda function was mapped to this new "all_text" column, which split each post into individual words, counted them, and added this count to a new column. Similarly, the Natural Language Toolkit [sentiment analysis module](https://www.nltk.org/_modules/nltk/sentiment/vader.html) was used to perform sentiment analysis on the text, and this sentiment score was then added to a new column. Data cleaned and features added, the DataFrame was exported to a new [CSV](https://github.com/Jack-Rayner/reddit-classification/blob/main/Datasets/reddit_posts_clean.csv). This dataset was prime for analysis, with visualizations generated for both sentiment and word count, and the top words from each subreddit extracted. This will be discussed in greater length in the Conclusions and Recommendations section.

#### Modeling
The first step in the modeling process was to read the clean data into the [notebook](https://github.com/Jack-Rayner/reddit-classification/blob/main/Notebooks/Modeling.ipynb). Unfortunately, Pandas reads in empty strings in a CSV as NumPy NaNs by default, so these had to be filled as empty strings. As one Subreddit had marginally more missing values than the other, about 52% of posts come from UnpopularOpinion, while around 48% come from AskScience. This means that when creating the null model, the model will predict all posts as coming from UnpopularOpinion, and be right only 52% of the time. In creating a model, *this is the number to beat*. The first model to be auditioned was Scikitlearn's [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). The X and y variables were then assigned to a DataFrame with the "subreddit" and "target" columns dropped and the column "subreddit" respectively. This was then filtered into training and testing data, in order to asses the accuracy of the model after training. In order to use the sentiment and word count as features for a GridSearchCV model, they must be processed separately from the text, which can be done using a [Feature Union](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html). These were then converted to functions, as they would not otherwise be able to be passed into the pipeline. In preparation for the model, the text column was run through a count vectorizer, which, for simplicities sake, represents words as ones or zeros in a matrix. All features were then combined in the second pipeline through the aforementioned, feature union, and then passed into the random forest model. Rather than hardcoding parameters into the model and hoping to find the ideal ones through trial and error, a dictionary of parameters was passed into a gridsearch model, which will test all combinations of parameters and returns only the best. This model was fitted, and then tested against the test date in order to determine accuracy. The model preformed at 89% accuracy, which is well above the null model. To get a more well rounded view, the true negatives and positives, as well as the false negatives and positives were plotted on a confusion matrix. As this is a complex model and there are many features, it is important to visualize the results of the model to truly understand its strengths and shortcomings. However, the only thing worse than a misleading graph is an ugly one, and so with some direction from [this article](https://towardsdatascience.com/beautiful-custom-colormaps-with-matplotlib-5bab3d1f0e72) a custom colormap was created for the confusion matrices.

While the performance of this model is satisfactory, the text was left relatively untouched before being fed to the model, leaving the opportunity to further optimize. When examining a word like "examining", the current model would have no idea that it has a relationship or derivation from the word "examine". To combat this, the text can be stemmed or lemmatized, both of which aim to reduce the word to its base form, and ideally get a more accurate count of a given word in the corpus. To implement this, the values for each parameter from the last gridsearch model were hardcoded into the model, as recalculating these parameters would unnecessarily increase run time. The stemmer and lemmatizer selected were the natural language tool kit [WordNetLemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html) and [PorterStemmer](https://www.nltk.org/howto/stem.html). Despite the theoretical benefits of these transformations, the model actually performed worse than the previous model, with a score of only 88%.




<br><br><br><br>
#### Methods (change to notebooks):
 Following this, the text was run through a count vectorizer. Through a process of trial and comparison, it was determined that a Logistic Regression model is the best choice for the prediction of this target. While the a Random Forest model both with and without stemming and lemitization performed above the null model (the mean of the target), it fell short of the accuracy of Logistic Regression.


### Data Dictionary:

|Feature|Type|Dataset|Description|
|---|---|---|---|
|**title**|*str*|reddit_posts_clean.csv|Title of Reddit post|
|**selftext**|*str*|reddit_posts_clean.csv|Text body of Reddit post|
|**subreddit**|*str*|reddit_posts_clean.csv|Subreddit the post was posted to|
|**created_utc**|*numpy.int64*|reddit_posts_clean.csv|Date and time the reddit post was created|
|**target**|*int*|reddit_posts_clean.csv|Binarized version of Subreddit column (0 being askscience and 1 being unpopularopinion)|
|**all_text**|*str*|reddit_posts_clean.csv|Text from title and selftext columns combined to one string|
|**word_count**|*integer*|reddit_posts_clean.csv|Total number of words in the post (both title and selftext)|
|**sentiment**|*float*|reddit_posts_clean.csv|The sentiment of the text in a given post, ranging from 1 to -1 (most positive and most negative respectivly)|


### Conclusions and Recommendations
Through the use of a Logistic Regression model, the added features of sentiment analysis and word count, and the count vectorization of the text, the price of a house can be predicted within a 10% margin of error. This fulfills the projected goal, and performs well above the null model, which predicts with only 52.31% accuracy. This analysis also allowed for discovery of key features such as which words act as the best predictors of each Subreddit, the distribution of sentiment by Subreddit, and the distribution of word count.
